{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed praw and pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praw reddit authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='80VRpM_i5PAB0w', client_secret='JrwCgUR0D0wVGgpdHH4imteSnI4', user_agent='scraper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polling r/India for top posts with submission object and saving the query results to a pandas DataFrame object for saving to a csv file for easy loading and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = []\n",
    "subreddit = reddit.subreddit('india')\n",
    "\n",
    "for post in subreddit.top(limit=10000):\n",
    "    post.comment_sort = 'top'\n",
    "    post.comments.replace_more(limit=0)\n",
    "    comments=[]\n",
    "    for comment in post.comments:\n",
    "        comments.append(comment.body)\n",
    "    comments = comments[:3]\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.link_flair_text, post.stickied, post.upvote_ratio, comments, post.distinguished, post.edited, post.over_18, post.locked, post.is_original_content, post.is_self])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'link_flair_text','stickied', 'upvote_ratio', 'comments', 'distinguished','edited','over_18','locked','is_original_content','is_self'])\n",
    "posts.to_csv('reddit.csv',index = False)\n",
    "len(posts.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that fewer than expected posts are returned by the query, this is partly due to the api being rate limited to 1000 posts at a time and api can't access posts older than a month.\n",
    "Also this method is very time intensive to poll all search criterias to get the data.\n",
    "\n",
    "To get more data we use Pushshift.io to get access to all posts of r/India without being limited in any way. We chose the limit by seeing the total number of posts <a href='https://search.pushshift.io/reddit/'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "result = api.search_submissions(subreddit='india',limit=1049762)\n",
    "\n",
    "df = pd.DataFrame([thing.d_ for thing in result])\n",
    "df.to_csv(r'D:\\Projects\\midas\\api.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we analyze the data extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
